name: Update Subscription

on:
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install dependencies
        run: pip install requests beautifulsoup4

      - name: Crawl and generate subscribe-new-pac
        run: |
          python - <<'PY'
          import requests
          from bs4 import BeautifulSoup
          import re
          import base64

          headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}

          # 1. Root page
          r = requests.get('https://freefq.com/v2ray/', headers=headers, timeout=30)
          r.raise_for_status()
          r.encoding = 'gb2312'  # Required for correct Chinese title detection
          soup = BeautifulSoup(r.text, 'html.parser')

          latest_url = None
          for a in soup.find_all('a', href=True):
              text = a.get_text(strip=True)
              if '免费v2ray账号分享' in text and a['href'].endswith('v2ray.html'):
                  href = a['href']
                  latest_url = 'https://freefq.com' + href if not href.startswith('http') else href
                  break

          if not latest_url:
              print("ERROR: No latest post found")
              exit(1)
          print(f"Latest post: {latest_url}")

          # 2. Post page
          r = requests.get(latest_url, headers=headers, timeout=30)
          r.raise_for_status()
          # Post page is UTF-8, but gb2312 works too (no harm)
          r.encoding = 'gb2312'
          soup = BeautifulSoup(r.text, 'html.parser')

          htm_url = None
          for a in soup.find_all('a', href=True):
              if a['href'].endswith('.htm'):
                  href = a['href']
                  htm_url = 'https://www.freefq.com' + href if not href.startswith('http') else href
                  break

          if not htm_url:
              print("ERROR: No .htm link found")
              exit(1)
          print(f"Nodes page: {htm_url}")

          # 3. .htm page - parse as HTML to strip all tags cleanly
          r = requests.get(htm_url, headers=headers, timeout=30)
          r.raise_for_status()
          # No charset header/meta, but content works fine with gb2312 or utf-8 (links are ASCII-compatible)
          r.encoding = 'gb2312'  # Safe choice
          soup = BeautifulSoup(r.text, 'html.parser')
          page_text = soup.get_text(separator='\n')

          links = []
          capture_next = False
          import_pattern = re.compile(r'^节点\d.*一键导入链接')
          link_pattern = re.compile(r'^\w+://')

          for line in page_text.splitlines():
              line = line.strip()
              if import_pattern.match(line):
                  capture_next = True
                  continue
              if capture_next and link_pattern.match(line):
                  links.append(line)
                  capture_next = False

          if not links:
              print("WARNING: No links extracted")

          print(f"Extracted {len(links)} links")

          # 4. Base64 each link individually
          b64_lines = [base64.b64encode(link.encode('utf-8')).decode('utf-8') for link in links]
          content = '\n'.join(b64_lines)

          with open('subscribe-new-pac', 'w', encoding='utf-8') as f:
              f.write(content)

          PY

      - name: Commit and push if changed
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add subscribe-new-pac
          if git diff --staged --quiet; then
            echo "No changes"
          else
            git commit -m "Update subscription from freefq.com"
            git push
          fi
